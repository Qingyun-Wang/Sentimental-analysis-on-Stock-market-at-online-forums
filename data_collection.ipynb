{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import yfinance as yf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up token and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Authentication\n",
    "reddit = praw.Reddit()\n",
    "\n",
    "subreddit = reddit.subreddit(\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_limited_request(subreddits, companies, effective_period, post_limit=50, comment_limit=20):\n",
    "    data = []\n",
    "    for subreddit_name in subreddits:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        for company in companies:\n",
    "            retry_wait = 20  # Initial wait time for 10 seconds\n",
    "            while True:\n",
    "                try:\n",
    "                    count_post = 0\n",
    "                    for post in subreddit.search(str(company), time_filter=effective_period):\n",
    "                        post_time = datetime.utcfromtimestamp(post.created_utc).strftime('%Y-%m-%d')\n",
    "                        data.append([post.title + \"\\n\" + post.selftext, post_time, company])\n",
    "                        count_post += 1\n",
    "                        count_comment = 0\n",
    "\n",
    "                        try:\n",
    "                            post.comments.replace_more(limit=0)\n",
    "                            for comment in post.comments:\n",
    "                                comment_time = datetime.utcfromtimestamp(comment.created_utc).strftime('%Y-%m-%d')\n",
    "                                data.append([comment.body, comment_time, company])\n",
    "                                count_comment += 1\n",
    "                                if count_comment > comment_limit:\n",
    "                                    break\n",
    "                        except praw.exceptions.RedditAPIException as e:\n",
    "                            print(f\"Error fetching comments: {e}\")\n",
    "                            time.sleep(retry_wait)\n",
    "                            retry_wait *= 2  # Exponential backoff\n",
    "\n",
    "                        if count_post > post_limit:\n",
    "                            break\n",
    "                    break  # Break the while loop if no exception occurred\n",
    "                except praw.exceptions.RedditAPIException as e:\n",
    "                    print(f\"Rate limit exceeded: {e}\")\n",
    "                    time.sleep(retry_wait)\n",
    "                    retry_wait *= 2  # Exponential backoff\n",
    "    return data\n",
    "\n",
    "def collect_data_reddit(companies, effective_preriod):\n",
    "    # List of stock market-related subreddits\n",
    "    subreddits = [\"stocks\", \"investing\", \"wallstreetbets\", \"StockMarket\", \"options\", \"SecurityAnalysis\", \"Daytrading\"]\n",
    "    data = rate_limited_request(subreddits, companies, effective_preriod)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get wordnet POS tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Get English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Filter out the stop words, non-letter tokens, and lemmatize\n",
    "    filtered_text = [word for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    #filtered_text = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "\n",
    "    # Rejoin filtered text\n",
    "    filtered_sentence = ' '.join(filtered_text)\n",
    "\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv_reddit(data, file_name):\n",
    "    df = pd.DataFrame(data, columns=[\"Text\", \"Date\", \"Company\"])\n",
    "    df['Text'] = df['Text'].apply(clean_text)\n",
    "    df = df.groupby(['Company','Date']).agg({\"Text\": lambda x: ' '.join(x.astype(str))}).reset_index()\n",
    "    df = df[df['Text']!=\"\"]\n",
    "    save_file_name = 'data/reddit_'+file_name+'.csv'\n",
    "    df.to_csv(save_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_company_name(text):\n",
    "    terms = ['Inc\\.', 'Corporation', 'Company', 'plc', 'Limited', ',', 'and', '\\.com', 'A/S', 'PLC', \"'s\"]\n",
    "    pattern = r'(?:' + '|'.join(terms) + ')'\n",
    "    # Replace the matched terms with an empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "def generate_top_100_company_name(path):\n",
    "    companies_100 = pd.read_csv(path, index_col=0)\n",
    "    companies_100['Company'] = companies_100['Company Name'].apply(clean_company_name).apply(lambda x: x.strip())\n",
    "    companies = list(companies_100['Company'].values)\n",
    "    return companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_company_post_to_csv(companies, period, output_path=\"fetched_post\"):\n",
    "    company_post = collect_data_reddit(companies, period)\n",
    "    save_to_csv_reddit(company_post, output_path)\n",
    "    \n",
    "def source_company_post_to_csv_TOP100(path_to_100, period):\n",
    "\n",
    "    companies = generate_top_100_company_name(path_to_100)\n",
    "    data1 = collect_data_reddit(companies[:20], period)\n",
    "    data2 = collect_data_reddit(companies[20:40], period)\n",
    "    time.sleep(600)\n",
    "    print(\"1\")\n",
    "    data3 = collect_data_reddit(companies[40:60], period)\n",
    "    data4 = collect_data_reddit(companies[40:80], period)\n",
    "    time.sleep(600)\n",
    "    print(\"2\")\n",
    "    data5 = collect_data_reddit(companies[80:100], period)\n",
    "    data = data1+data2+data3+data4+data5\n",
    "    save_to_csv_reddit(data,\"fetched_post_top_100\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = generate_top_100_company_name(\"data/top100_companies_data.csv\")\n",
    "data1 = collect_data_reddit(companies[:20], \"month\")\n",
    "data2 = collect_data_reddit(companies[20:40], \"month\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = collect_data_reddit(companies[40:60], \"month\")\n",
    "data4 = collect_data_reddit(companies[40:80], \"month\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = collect_data_reddit(companies[80:100], \"month\")\n",
    "data = data1+data2+data3+data4+data5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv_reddit(data,\"fetched_post_top_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TooManyRequests",
     "evalue": "received 429 HTTP response",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msource_company_post_to_csv_TOP100\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/top100_companies_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmonth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 10\u001b[0m, in \u001b[0;36msource_company_post_to_csv_TOP100\u001b[0;34m(path_to_100, period)\u001b[0m\n\u001b[1;32m      8\u001b[0m data4 \u001b[38;5;241m=\u001b[39m collect_data_reddit(companies[\u001b[38;5;241m40\u001b[39m:\u001b[38;5;241m80\u001b[39m], period)\n\u001b[1;32m      9\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m data5 \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_data_reddit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompanies\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperiod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m data \u001b[38;5;241m=\u001b[39m data1\u001b[38;5;241m+\u001b[39mdata2\u001b[38;5;241m+\u001b[39mdata3\u001b[38;5;241m+\u001b[39mdata4\u001b[38;5;241m+\u001b[39mdata5\n\u001b[1;32m     12\u001b[0m save_to_csv_reddit(data,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfetched_post_top_100\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[40], line 41\u001b[0m, in \u001b[0;36mcollect_data_reddit\u001b[0;34m(companies, effective_preriod)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect_data_reddit\u001b[39m(companies, effective_preriod):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# List of stock market-related subreddits\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     subreddits \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstocks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvesting\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwallstreetbets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStockMarket\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSecurityAnalysis\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDaytrading\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 41\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mrate_limited_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubreddits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompanies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_preriod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Cell \u001b[0;32mIn[40], line 17\u001b[0m, in \u001b[0;36mrate_limited_request\u001b[0;34m(subreddits, companies, effective_period, post_limit, comment_limit)\u001b[0m\n\u001b[1;32m     14\u001b[0m count_comment \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mpost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomments\u001b[49m\u001b[38;5;241m.\u001b[39mreplace_more(limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m comment \u001b[38;5;129;01min\u001b[39;00m post\u001b[38;5;241m.\u001b[39mcomments:\n\u001b[1;32m     19\u001b[0m         comment_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mutcfromtimestamp(comment\u001b[38;5;241m.\u001b[39mcreated_utc)\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Dropbox/job/TheDataIncubator/capstone/venv/lib/python3.9/site-packages/praw/models/reddit/base.py:35\u001b[0m, in \u001b[0;36mRedditBase.__getattr__\u001b[0;34m(self, attribute)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the value of ``attribute``.\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attribute\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetched:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attribute)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattribute\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m~/Dropbox/job/TheDataIncubator/capstone/venv/lib/python3.9/site-packages/praw/models/reddit/submission.py:712\u001b[0m, in \u001b[0;36mSubmission._fetch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 712\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m     submission_listing, comment_listing \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    714\u001b[0m     comment_listing \u001b[38;5;241m=\u001b[39m Listing(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reddit, _data\u001b[38;5;241m=\u001b[39mcomment_listing[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Dropbox/job/TheDataIncubator/capstone/venv/lib/python3.9/site-packages/praw/models/reddit/submission.py:731\u001b[0m, in \u001b[0;36mSubmission._fetch_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_additional_fetch_params\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m    730\u001b[0m path \u001b[38;5;241m=\u001b[39m API_PATH[name]\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfields)\n\u001b[0;32m--> 731\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reddit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/job/TheDataIncubator/capstone/venv/lib/python3.9/site-packages/praw/util/deprecate_args.py:43\u001b[0m, in \u001b[0;36m_deprecate_args.<locals>.wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     arg_string \u001b[38;5;241m=\u001b[39m _generate_arg_string(_old_args[: \u001b[38;5;28mlen\u001b[39m(args)])\n\u001b[1;32m     37\u001b[0m     warn(\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional arguments for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m will no longer be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m supported in PRAW 8.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCall this function with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     42\u001b[0m     )\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_old_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/job/TheDataIncubator/capstone/venv/lib/python3.9/site-packages/praw/reddit.py:941\u001b[0m, in \u001b[0;36mReddit.request\u001b[0;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ClientException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt most one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 941\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequest \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m    950\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Dropbox/job/TheDataIncubator/capstone/venv/lib/python3.9/site-packages/prawcore/sessions.py:328\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[1;32m    326\u001b[0m     json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m url \u001b[38;5;241m=\u001b[39m urljoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requestor\u001b[38;5;241m.\u001b[39moauth_url, path)\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/job/TheDataIncubator/capstone/venv/lib/python3.9/site-packages/prawcore/sessions.py:267\u001b[0m, in \u001b[0;36mSession._request_with_retries\u001b[0;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_retry(\n\u001b[1;32m    255\u001b[0m         data,\n\u001b[1;32m    256\u001b[0m         files,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m         url,\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATUS_EXCEPTIONS:\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSTATUS_EXCEPTIONS[response\u001b[38;5;241m.\u001b[39mstatus_code](response)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m codes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_content\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTooManyRequests\u001b[0m: received 429 HTTP response"
     ]
    }
   ],
   "source": [
    "source_company_post_to_csv_TOP100(\"data/top100_companies_data.csv\",\"month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_today_post_top100(previous_post_path, path_to_100):\n",
    "    \"\"\"\n",
    "    update with the new post from last update\n",
    "    \"\"\"\n",
    "    previous_data = pd.read_csv(previous_post_path, index_col=0)\n",
    "    last_date_string = previous_data[\"Date\"].max()\n",
    "    last_date_time = datetime.strptime(last_date_string, '%Y-%m-%d')\n",
    "    today_time = datetime.today()\n",
    "    day_diff = (today_time - last_date_time).days\n",
    "    if day_diff>0:\n",
    "        if day_diff <=1:\n",
    "            update_period = \"day\"\n",
    "            sleep_time = 60\n",
    "            #date_one_days_ago = today_time - timedelta(days=1)\n",
    "            # Convert the date back to a string\n",
    "            today_time_string = today_time.strftime('%Y-%m-%d')\n",
    "            previous_data_correted = previous_data[previous_data['Date']< today_time_string]\n",
    "\n",
    "        elif day_diff <= 7 and day_diff > 1:\n",
    "            update_period = \"week\"\n",
    "            sleep_time = 120\n",
    "            # Subtract 7 days\n",
    "            date_seven_days_ago = today_time - timedelta(days=7)\n",
    "            # Convert the date back to a string\n",
    "            date_seven_days_ago_str = date_seven_days_ago.strftime('%Y-%m-%d')\n",
    "            previous_data_correted = previous_data[previous_data['Date']< date_seven_days_ago_str]\n",
    "\n",
    "        elif day_diff>7 and day_diff<=30:\n",
    "            update_period = 'month'\n",
    "            sleep_time = 300\n",
    "            date_30_days_ago = today_time - timedelta(days=30)\n",
    "            # Convert the date back to a string\n",
    "            date_30_days_ago = date_30_days_ago.strftime('%Y-%m-%d')\n",
    "            previous_data_correted = previous_data[previous_data['Date']< date_30_days_ago]\n",
    "\n",
    "        \n",
    "        companies = generate_top_100_company_name(path_to_100)\n",
    "        print('start fetch', update_period)\n",
    "        data1 = collect_data_reddit(companies[:20], update_period)\n",
    "        data2 = collect_data_reddit(companies[20:40], update_period)\n",
    "        print('fetchign -1', f\"sleep at {datetime.today()}\")\n",
    "        time.sleep(sleep_time)\n",
    "        print(\"end sleep\", f\"wakeup at {datetime.today()}\")\n",
    "        data3 = collect_data_reddit(companies[40:60], update_period)\n",
    "        data4 = collect_data_reddit(companies[40:80], update_period)\n",
    "        print('fetchign -2')\n",
    "        time.sleep(sleep_time)\n",
    "        data5 = collect_data_reddit(companies[80:100], update_period)\n",
    "        print('fetched')\n",
    "        data = data1+data2+data3+data4+data5\n",
    "        df = pd.DataFrame(data, columns=[\"Text\", \"Date\", \"Company\"])\n",
    "        df['Text'] = df['Text'].apply(clean_text)\n",
    "        df = df.groupby(['Company','Date']).agg({\"Text\": lambda x: ' '.join(x.astype(str))}).reset_index()\n",
    "        df = df[df['Text']!=\"\"]\n",
    "\n",
    "        data_updated = pd.concat([previous_data_correted, df])\n",
    "\n",
    "        save_to_csv_reddit(data_updated,\"reddit_fetched_post_top_100\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stock_info(path_to_100_company = \"data/top100_companies_data.csv\"):\n",
    "\n",
    "    symbols = generate_top_100_company_name(path_to_100_company, index_col=0)\n",
    "    # Prepare a DataFrame to store the results\n",
    "    stock_info = pd.DataFrame(columns=[\"Symbol\",\"Company Name\", \"% Change\"])\n",
    "\n",
    "    # Loop through each symbol\n",
    "    for symbol in symbols:\n",
    "        # Fetch data for the stock\n",
    "        stock = yf.Ticker(symbol)\n",
    "\n",
    "        # Get the company name\n",
    "        company_name = stock.info.get('longName', 'Unknown')\n",
    "\n",
    "        # Get historical data for the stock\n",
    "        hist = stock.history(period=\"1d\")\n",
    "\n",
    "        # Calculate the percentage change, change to form percentage% inorder to make old code\n",
    "        if not hist.empty:\n",
    "            percent_change = str(((hist['Close'] - hist['Open']) / hist['Open'] * 100).iloc[-1])\n",
    "        else:\n",
    "            percent_change = None\n",
    "\n",
    "        # Create a temporary DataFrame and handle NA values\n",
    "        df2 = pd.DataFrame({\n",
    "            \"Symbol\": symbol,\n",
    "            \"Company Name\": [company_name] if company_name is not None else [pd.NA],\n",
    "            \"% Change\": [percent_change] if percent_change is not None else [pd.NA]\n",
    "        })\n",
    "\n",
    "        # Append to the DataFrame\n",
    "        stock_info = pd.concat([stock_info, df2], ignore_index=True)\n",
    "\n",
    "    stock_info.to_csv('data/top100_companies_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BRK.B: No data found, symbol may be delisted\n"
     ]
    }
   ],
   "source": [
    "symbols_list = pd.read_csv('data/top100_companies_data.csv')['Symbol'].values\n",
    "stock_data = get_stock_info(symbols_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.to_csv('data/top100_companies_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stock_data['% Change'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>% Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>0.04544810420607641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>Microsoft Corporation</td>\n",
       "      <td>-1.9769105920218903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GOOGL</td>\n",
       "      <td>Alphabet Inc.</td>\n",
       "      <td>-1.0796239231445781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com, Inc.</td>\n",
       "      <td>-1.6741110047698855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>NVIDIA Corporation</td>\n",
       "      <td>-0.08266044668930941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>SBUX</td>\n",
       "      <td>Starbucks Corporation</td>\n",
       "      <td>-0.891414983722619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>SONY</td>\n",
       "      <td>Sony Group Corporation</td>\n",
       "      <td>1.2267196930459867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>PLD</td>\n",
       "      <td>Prologis, Inc.</td>\n",
       "      <td>3.5180342496785184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>BKNG</td>\n",
       "      <td>Booking Holdings Inc.</td>\n",
       "      <td>-0.39375355106256554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>SYK</td>\n",
       "      <td>Stryker Corporation</td>\n",
       "      <td>-0.6354494637071488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Symbol            Company Name              % Change\n",
       "0    AAPL              Apple Inc.   0.04544810420607641\n",
       "1    MSFT   Microsoft Corporation   -1.9769105920218903\n",
       "2   GOOGL           Alphabet Inc.   -1.0796239231445781\n",
       "3    AMZN        Amazon.com, Inc.   -1.6741110047698855\n",
       "4    NVDA      NVIDIA Corporation  -0.08266044668930941\n",
       "..    ...                     ...                   ...\n",
       "95   SBUX   Starbucks Corporation    -0.891414983722619\n",
       "96   SONY  Sony Group Corporation    1.2267196930459867\n",
       "97    PLD          Prologis, Inc.    3.5180342496785184\n",
       "98   BKNG   Booking Holdings Inc.  -0.39375355106256554\n",
       "99    SYK     Stryker Corporation   -0.6354494637071488\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
